[
  {
    "path": "posts/2021-08-21-replicating-a-fraud-detection/",
    "title": "Replicating a fraud detection analysis",
    "description": "A blog post suggested evidence of fraud in an influential field experiment. I replicate their analysis and share the code.",
    "author": [
      {
        "name": "Avishai M Tsur",
        "url": {}
      }
    ],
    "date": "2021-08-21",
    "categories": [],
    "contents": "\r\nIntroduction\r\nIn recent days, Psychology and Business Management twitter is on fire. Not due to the COVID-19 pandemic, exiting Afghanistan, or climate change, but due to evidence of flaws in a highly cited paper from 2012 titled: “Signing at the beginning makes ethics salient and decreases dishonest self-reports in comparison to signing at the end”.\r\nFor some context see:\r\n- Datacolada\r\n- PNAS 2012 paper\r\n- PNAS 2020 paper\r\nIn this post, I will try to replicate some of the findings of Prof. Uri Simonsohn, Prof. Leif D. Nelson, Prof. Joe Simmons and unknown others visualizing the concerns with one of the experiments in the study that originally claimed dishonesty can be reduced by asking people to sign a statement of honest intent before providing information (i.e., at the top of a document) rather than after providing information (i.e., at the bottom of a document).\r\nIf you are an Rstats user, I hope for you this would be a quick peek to some of the methods used (in R) to detect errors, frauds, and miscalculations in scientific literature.\r\nA few important disclaimers\r\nIt is highly suggested to read the Datacolada blog post first and keep it open while reading this.\r\nI am not a psychologist, a fraud detection expert, nor business expert. My pure intention is to replicate the analysis for there to be an open-sourced code so others could replicate it as well and have a reference for future use.\r\nNo part of this post suggests responsibility, bad intentions, or blame.\r\nMistakes happen all the time, even in this post.\r\nThe replication is not a full replication, but of the main findings.\r\nIt is much easier to replicate the analysis when it is well explained, based on open data. Kudos to the known and unknown authors of the Datacolada blog, and to the authors of the 2012 and 2020 papers for their contribution to science and truth.\r\nAlso, it is much more difficult to conduct a fraud analysis when nobody pointed the flaws for you. It is very likely that most of the analyses that they performed and did not raise suspicion was not published at all. So do not assume it is easy.\r\nEnglish is not my primary language, so excuse any mistakes.\r\nData source\r\nThe data used for this post is available at this link in a file named: DrivingdataAll(1).xls.\r\nAs you will see later, for technical reasons I opened the file in MS Excel and saved it as an .xlsx file.\r\nQuick and dirty description of the experimental design\r\nThe experiment (#3 in the original paper) used data from a car insurance company. Customers were randomized to receive a policy with an honesty statement at the top or at the bottom of the form. “The first odometer reading was based on the mileage information the insurance company previously had on file, whereas the second was the current odometer reading that customers reported (and had an incentive to report smaller numbers - AMT).” To make a long story short, the conclusion was “a simple change in the location of a signature request can significantly influence the extent to which people on average will misreport information to advance their own self-interest.”\r\nProcessing\r\nRead the data\r\nFirst, let us read the data and change columns names for consistency, comfort, and brevity.\r\n\r\n\r\ndrv <- read_xlsx(\"Data/DrivingdataAll.xlsx\") %>% \r\n    janitor::clean_names() %>% \r\n    rename(arm = \"omr_version\",\r\n           pid = \"policy_number_masked\",\r\n           car_1_pre = \"odom_reading_1_previous\",\r\n           car_1_post = \"odom_reading_1_update\",\r\n           car_2_pre = \"odom_reading_2_previous\",\r\n           car_2_post = \"odom_reading_2_update\",\r\n           car_3_pre = \"odom_reading_3_previous\",\r\n           car_3_post = \"odom_reading_3_update\",\r\n           car_4_pre = \"odom_reading_4_previous\",\r\n           car_4_post = \"odom_reading_4_update\",\r\n           avg_pre = \"odom_reading_all_previous\",\r\n           avg_post = \"odom_reading_all_update\",\r\n           avg_diff = \"diff_all\",\r\n           ind1 = \"count1\",\r\n           ind2 = \"count2\",\r\n           ind3 = \"count3\",\r\n           ind4 = \"count4\",\r\n           n_cars = \"number_cars_in_policy\"\r\n           )\r\n\r\n\r\n\r\nQuick inspection\r\n\r\n\r\nsample_n(drv, 10) %>% paged_table()\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"arm\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"pid\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"car_1_pre\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"car_1_post\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"car_2_pre\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"car_2_post\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"car_3_pre\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"car_3_post\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"car_4_pre\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"car_4_post\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"avg_pre\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"avg_post\"],\"name\":[12],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"avg_diff\"],\"name\":[13],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ind1\"],\"name\":[14],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ind2\"],\"name\":[15],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ind3\"],\"name\":[16],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"ind4\"],\"name\":[17],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"n_cars\"],\"name\":[18],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Sign Top\",\"2\":\"8544\",\"3\":\"72000\",\"4\":\"99649\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"NA\",\"8\":\"NA\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"72000\",\"12\":\"99649.0\",\"13\":\"27649.0\",\"14\":\"1\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"1\"},{\"1\":\"Sign Bottom\",\"2\":\"5969\",\"3\":\"70837\",\"4\":\"118704\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"NA\",\"8\":\"NA\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"70837\",\"12\":\"118704.0\",\"13\":\"47867.0\",\"14\":\"1\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"1\"},{\"1\":\"Sign Bottom\",\"2\":\"11640\",\"3\":\"50\",\"4\":\"628\",\"5\":\"60998\",\"6\":\"62753\",\"7\":\"NA\",\"8\":\"NA\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"30524\",\"12\":\"31690.5\",\"13\":\"1166.5\",\"14\":\"1\",\"15\":\"1\",\"16\":\"0\",\"17\":\"0\",\"18\":\"2\"},{\"1\":\"Sign Bottom\",\"2\":\"12469\",\"3\":\"93755\",\"4\":\"138380\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"NA\",\"8\":\"NA\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"93755\",\"12\":\"138380.0\",\"13\":\"44625.0\",\"14\":\"1\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"1\"},{\"1\":\"Sign Top\",\"2\":\"4409\",\"3\":\"192300\",\"4\":\"197050\",\"5\":\"125250\",\"6\":\"144597\",\"7\":\"NA\",\"8\":\"NA\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"158775\",\"12\":\"170823.5\",\"13\":\"12048.5\",\"14\":\"1\",\"15\":\"1\",\"16\":\"0\",\"17\":\"0\",\"18\":\"2\"},{\"1\":\"Sign Top\",\"2\":\"13091\",\"3\":\"514\",\"4\":\"30060\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"NA\",\"8\":\"NA\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"514\",\"12\":\"30060.0\",\"13\":\"29546.0\",\"14\":\"1\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"1\"},{\"1\":\"Sign Top\",\"2\":\"8413\",\"3\":\"20471\",\"4\":\"22190\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"NA\",\"8\":\"NA\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"20471\",\"12\":\"22190.0\",\"13\":\"1719.0\",\"14\":\"1\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"1\"},{\"1\":\"Sign Bottom\",\"2\":\"5984\",\"3\":\"47004\",\"4\":\"54799\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"NA\",\"8\":\"NA\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"47004\",\"12\":\"54799.0\",\"13\":\"7795.0\",\"14\":\"1\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"1\"},{\"1\":\"Sign Top\",\"2\":\"9198\",\"3\":\"19485\",\"4\":\"58663\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"NA\",\"8\":\"NA\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"19485\",\"12\":\"58663.0\",\"13\":\"39178.0\",\"14\":\"1\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"1\"},{\"1\":\"Sign Bottom\",\"2\":\"13338\",\"3\":\"98841\",\"4\":\"144440\",\"5\":\"NA\",\"6\":\"NA\",\"7\":\"NA\",\"8\":\"NA\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"98841\",\"12\":\"144440.0\",\"13\":\"45599.0\",\"14\":\"1\",\"15\":\"0\",\"16\":\"0\",\"17\":\"0\",\"18\":\"1\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nFor each policy there is a row with up to four cars with first and last odometer readings.\r\nThe clue\r\nOne early clue of what is about to come was mentioned in the PNAS 2020 paper. Despite randomization, baseline readings were significantly different.\r\n\r\n\r\ndrv %>% select(avg_pre, avg_post, avg_diff, arm) %>% \r\n    tbl_summary(by = arm, \r\n                statistic = list(all_continuous() ~ \"{mean} ({sd})\"), \r\n                digits = list(all_continuous() ~ c(2,2))) %>% \r\n    add_p(test = list(all_continuous() ~ \"t.test\"))\r\n\r\n\r\n\r\nhtml {\r\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r\n}\r\n\r\n#affxydzdzu .gt_table {\r\n  display: table;\r\n  border-collapse: collapse;\r\n  margin-left: auto;\r\n  margin-right: auto;\r\n  color: #333333;\r\n  font-size: 16px;\r\n  font-weight: normal;\r\n  font-style: normal;\r\n  background-color: #FFFFFF;\r\n  width: auto;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #A8A8A8;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #A8A8A8;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n}\r\n\r\n#affxydzdzu .gt_heading {\r\n  background-color: #FFFFFF;\r\n  text-align: center;\r\n  border-bottom-color: #FFFFFF;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#affxydzdzu .gt_title {\r\n  color: #333333;\r\n  font-size: 125%;\r\n  font-weight: initial;\r\n  padding-top: 4px;\r\n  padding-bottom: 4px;\r\n  border-bottom-color: #FFFFFF;\r\n  border-bottom-width: 0;\r\n}\r\n\r\n#affxydzdzu .gt_subtitle {\r\n  color: #333333;\r\n  font-size: 85%;\r\n  font-weight: initial;\r\n  padding-top: 0;\r\n  padding-bottom: 6px;\r\n  border-top-color: #FFFFFF;\r\n  border-top-width: 0;\r\n}\r\n\r\n#affxydzdzu .gt_bottom_border {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#affxydzdzu .gt_col_headings {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#affxydzdzu .gt_col_heading {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 6px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#affxydzdzu .gt_column_spanner_outer {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: normal;\r\n  text-transform: inherit;\r\n  padding-top: 0;\r\n  padding-bottom: 0;\r\n  padding-left: 4px;\r\n  padding-right: 4px;\r\n}\r\n\r\n#affxydzdzu .gt_column_spanner_outer:first-child {\r\n  padding-left: 0;\r\n}\r\n\r\n#affxydzdzu .gt_column_spanner_outer:last-child {\r\n  padding-right: 0;\r\n}\r\n\r\n#affxydzdzu .gt_column_spanner {\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: bottom;\r\n  padding-top: 5px;\r\n  padding-bottom: 5px;\r\n  overflow-x: hidden;\r\n  display: inline-block;\r\n  width: 100%;\r\n}\r\n\r\n#affxydzdzu .gt_group_heading {\r\n  padding: 8px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#affxydzdzu .gt_empty_group_heading {\r\n  padding: 0.5px;\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  vertical-align: middle;\r\n}\r\n\r\n#affxydzdzu .gt_from_md > :first-child {\r\n  margin-top: 0;\r\n}\r\n\r\n#affxydzdzu .gt_from_md > :last-child {\r\n  margin-bottom: 0;\r\n}\r\n\r\n#affxydzdzu .gt_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  margin: 10px;\r\n  border-top-style: solid;\r\n  border-top-width: 1px;\r\n  border-top-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 1px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 1px;\r\n  border-right-color: #D3D3D3;\r\n  vertical-align: middle;\r\n  overflow-x: hidden;\r\n}\r\n\r\n#affxydzdzu .gt_stub {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  font-size: 100%;\r\n  font-weight: initial;\r\n  text-transform: inherit;\r\n  border-right-style: solid;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n  padding-left: 12px;\r\n}\r\n\r\n#affxydzdzu .gt_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#affxydzdzu .gt_first_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#affxydzdzu .gt_grand_summary_row {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  text-transform: inherit;\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n}\r\n\r\n#affxydzdzu .gt_first_grand_summary_row {\r\n  padding-top: 8px;\r\n  padding-bottom: 8px;\r\n  padding-left: 5px;\r\n  padding-right: 5px;\r\n  border-top-style: double;\r\n  border-top-width: 6px;\r\n  border-top-color: #D3D3D3;\r\n}\r\n\r\n#affxydzdzu .gt_striped {\r\n  background-color: rgba(128, 128, 128, 0.05);\r\n}\r\n\r\n#affxydzdzu .gt_table_body {\r\n  border-top-style: solid;\r\n  border-top-width: 2px;\r\n  border-top-color: #D3D3D3;\r\n  border-bottom-style: solid;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n}\r\n\r\n#affxydzdzu .gt_footnotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#affxydzdzu .gt_footnote {\r\n  margin: 0px;\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#affxydzdzu .gt_sourcenotes {\r\n  color: #333333;\r\n  background-color: #FFFFFF;\r\n  border-bottom-style: none;\r\n  border-bottom-width: 2px;\r\n  border-bottom-color: #D3D3D3;\r\n  border-left-style: none;\r\n  border-left-width: 2px;\r\n  border-left-color: #D3D3D3;\r\n  border-right-style: none;\r\n  border-right-width: 2px;\r\n  border-right-color: #D3D3D3;\r\n}\r\n\r\n#affxydzdzu .gt_sourcenote {\r\n  font-size: 90%;\r\n  padding: 4px;\r\n}\r\n\r\n#affxydzdzu .gt_left {\r\n  text-align: left;\r\n}\r\n\r\n#affxydzdzu .gt_center {\r\n  text-align: center;\r\n}\r\n\r\n#affxydzdzu .gt_right {\r\n  text-align: right;\r\n  font-variant-numeric: tabular-nums;\r\n}\r\n\r\n#affxydzdzu .gt_font_normal {\r\n  font-weight: normal;\r\n}\r\n\r\n#affxydzdzu .gt_font_bold {\r\n  font-weight: bold;\r\n}\r\n\r\n#affxydzdzu .gt_font_italic {\r\n  font-style: italic;\r\n}\r\n\r\n#affxydzdzu .gt_super {\r\n  font-size: 65%;\r\n}\r\n\r\n#affxydzdzu .gt_footnote_marks {\r\n  font-style: italic;\r\n  font-weight: normal;\r\n  font-size: 65%;\r\n}\r\nCharacteristic\r\n      Sign Bottom, N = 6,6641\r\n      Sign Top, N = 6,8241\r\n      p-value2\r\n    avg_pre\r\n75,034.50 (50,265.35)\r\n59,692.71 (49,953.51)\r\n<0.001avg_post\r\n98,705.14 (51,934.76)\r\n85,791.10 (51,701.31)\r\n<0.001avg_diff\r\n23,670.64 (12,621.38)\r\n26,098.40 (12,253.37)\r\n<0.001\r\n        \r\n          1\r\n          \r\n           \r\n          Mean (SD)\r\n          \r\n        \r\n          2\r\n          \r\n           \r\n          Welch Two Sample t-test\r\n          \r\n      \r\n    \r\n\r\nIt is unknown to me why numbers were presented to the second decimal digit (in the paper), but I left it here as it is irrelevant for the topic.\r\nPrepping\r\nI will refer to some pitfalls of using spreadsheet applications for processing data later, but for now, let us pivot the table to a more tidy format. This reshaping would ease visualizing. Notice how I try to conduct most if not all processing in an R or Rmarkdown script. This way, each step is documented and can be more easily replicated. This also allows correcting early mistakes without having to redo all of the analysis manually.\r\n\r\n\r\ndrv_long <- drv %>% \r\n    select(arm, pid, starts_with(\"car\")) %>% \r\n    pivot_longer(cols = starts_with(\"car\"), names_to = \"car\", values_to = \"miles\") %>% \r\n    na.omit() %>% \r\n    extract(col = car, \r\n            into = c(\"car_num\", \"period\"), \r\n            regex = \"car_([[:digit:]])_([[:alpha:]]*)\") %>% \r\n    mutate(car_num = paste0(\"Car \", car_num),\r\n           period = str_to_title(period),\r\n           period = factor(period) %>% fct_rev())\r\n\r\nsample_n(drv_long, 10) %>% paged_table()\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"arm\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"pid\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"car_num\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"period\"],\"name\":[4],\"type\":[\"fct\"],\"align\":[\"left\"]},{\"label\":[\"miles\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Sign Bottom\",\"2\":\"2369\",\"3\":\"Car 1\",\"4\":\"Post\",\"5\":\"65579\"},{\"1\":\"Sign Top\",\"2\":\"776\",\"3\":\"Car 1\",\"4\":\"Post\",\"5\":\"45274\"},{\"1\":\"Sign Top\",\"2\":\"4708\",\"3\":\"Car 2\",\"4\":\"Pre\",\"5\":\"15000\"},{\"1\":\"Sign Top\",\"2\":\"8368\",\"3\":\"Car 1\",\"4\":\"Post\",\"5\":\"9800\"},{\"1\":\"Sign Top\",\"2\":\"7183\",\"3\":\"Car 2\",\"4\":\"Pre\",\"5\":\"14262\"},{\"1\":\"Sign Top\",\"2\":\"7027\",\"3\":\"Car 3\",\"4\":\"Pre\",\"5\":\"46547\"},{\"1\":\"Sign Bottom\",\"2\":\"12005\",\"3\":\"Car 1\",\"4\":\"Pre\",\"5\":\"111000\"},{\"1\":\"Sign Bottom\",\"2\":\"2401\",\"3\":\"Car 2\",\"4\":\"Pre\",\"5\":\"125876\"},{\"1\":\"Sign Bottom\",\"2\":\"7553\",\"3\":\"Car 4\",\"4\":\"Post\",\"5\":\"197735\"},{\"1\":\"Sign Top\",\"2\":\"917\",\"3\":\"Car 3\",\"4\":\"Pre\",\"5\":\"26\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nDisturbing Major Anomalies\r\nAnomaly #1: Implausible Distribution of Miles Driven\r\nThe distribution of miles driven is expected to be somewhat of a positively skewed normal distribution. Miles cannot be below 0 but can go very high in rare cases. The Datacolada blog post authors go out of their way and find a similar study design to show that this expectation is not unrealistic.\r\n\r\n\r\ndrv_long %>% \r\n    arrange(pid, car_num, period) %>% \r\n    group_by(pid, car_num) %>% \r\n    summarize(miles_diff = diff(miles), .groups = \"drop\") %>% \r\n    ggplot(aes(miles_diff, fill = car_num)) +\r\n    geom_histogram(binwidth = 2000, center = 1000, show.legend = F, col = \"black\") +\r\n    facet_wrap(~car_num, scales = \"free_y\") +\r\n    scale_x_continuous(limits = c(0,60000), breaks = seq(0, 75e3, 1e4),\r\n                       labels = comma_format()) +\r\n    labs(x = \"Miles driven\", y = \"Frequency\", \r\n         title = \"A strange distribution and 50K cut-off\",\r\n         subtitle = \"Miles driven histogram by # of car\")\r\n\r\n\r\n\r\n\r\nHowever, the miles driven have a uniform distribution with many cars almost reaching 50K but no cars at all above. The appearance of this cut-off in all cars raises even more suspicion.\r\nAnomaly #2: No Rounded Mileages At Time 2\r\nWhen human report on numbers, rounding is not uncommon. In many fraud detection cases, the round numbers are evidence of human intervention and therefore, possible signs of fraud (by the human reporting the numbers). In our case however, customer rounding odometer reading are not a problem and are actually expected. The diminished frequency of round numbers in the second odometer reading (supposedly reported by the customers) suggests of at least a possible mistake in data processing. The existence of rounding in the first odometer reading suggest an unexplained inconsistency.\r\n\r\n\r\ndrv_long %>% \r\n    mutate(last_digits = miles %% 1e3) %>% \r\n    group_by(period, last_digits) %>% \r\n    summarize(n = n()) %>% \r\n    mutate(pct = n / sum(n),\r\n           mult_100 = last_digits %% 100 == 0) %>% \r\n    ungroup() %>% \r\n    arrange(mult_100) %>% \r\n    ggplot(aes(last_digits, pct, col = mult_100)) +\r\n    geom_point(show.legend = F) +\r\n    scale_color_manual(values = c(\"black\", \"red\")) +\r\n    scale_y_continuous(labels = percent_format(accuracy = 1)) +\r\n    scale_x_continuous(breaks = c(0,200,400,600,800,999), \r\n                       labels = c(\"000\", 200, 400, 600, 800, 999)) +\r\n    facet_wrap(~period) +\r\n    labs(x = \"Last 3 digits\", y = \"Percent of readings\",\r\n         title = \"Inconsistency in number rounding\",\r\n         subtitle = \"Percent of reading with specific 3 last digits\"\r\n         )\r\n\r\n\r\n\r\n\r\nThis number rounding can also be examined on a smaller scale (last digit).\r\n\r\n\r\ndrv_long %>% \r\n    mutate(last_digits = miles %% 1e1) %>% \r\n    group_by(period, last_digits) %>% \r\n    summarize(n = n()) %>% \r\n    mutate(pct = n / sum(n)) %>% \r\n    ungroup() %>% \r\n    ggplot(aes(last_digits, pct)) +\r\n    geom_col(col = \"black\", fill = \"lightblue\") +\r\n    facet_wrap(~period) +\r\n    scale_x_continuous(breaks = 0:9) +\r\n    scale_y_continuous(labels = percent_format(accuracy = 1)) +\r\n    labs(x = \"Last digit\", y = \"Percent of readings\", \r\n         title = \"Inconsistency in number rounding - Part 2\",\r\n         subtitle = \"Percent of reading with specific last digit\")\r\n\r\n\r\n\r\n\r\nAnomaly #3: Near-Duplicate Calibri and Cambria Observations\r\nThis next part is challenging. Files with the extension .xls or .xlsx can be filled with metadata presented in fonts, color-coding, comments, and others. These are frowned upon yet exist widespread. For elaboration why and what to do instead see this on Data Organization in Spreadsheets.\r\nIn our case, there is a suspicion the some data manipulation is apparent in different fonts in the column with the first odometer reading of the first car. Some are in Cambria font and some are in Calibri. Actually exactly half each. To check for this issue we need to read not the data, but the metadata. This is available using the tidyxl package, but only for .xlsx files. This is the reason we opened the file in MS Excel and changed the extension at the beginning of this post.\r\n\r\n\r\nlibrary(tidyxl)\r\n\r\ndrv_fmt <- xlsx_cells(\"Data/DrivingdataAll.xlsx\")\r\ndrv_fmt2 <- xlsx_formats(\"Data/DrivingdataAll.xlsx\")\r\n\r\n\r\n\r\nThe xlsx cells() function is a dataframe with a row for each cell in the original spreadsheet. The xlsx_formats() function is a list with information regarding types of formatting found in the spreadsheet. Their combination allows for detection of the font (and other properties) in each cell. Here we extract the suspected format with the Cambria font.\r\n\r\n\r\ndrv_fmt2$local$font$name %>% enframe() %>% paged_table()\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"name\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"value\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"1\",\"2\":\"Calibri\"},{\"1\":\"2\",\"2\":\"Calibri\"},{\"1\":\"3\",\"2\":\"Cambria\"},{\"1\":\"4\",\"2\":\"Calibri\"},{\"1\":\"5\",\"2\":\"Calibri\"},{\"1\":\"6\",\"2\":\"Calibri\"},{\"1\":\"7\",\"2\":\"Calibri\"},{\"1\":\"8\",\"2\":\"Calibri\"},{\"1\":\"9\",\"2\":\"Calibri\"},{\"1\":\"10\",\"2\":\"Calibri\"},{\"1\":\"11\",\"2\":\"Calibri\"},{\"1\":\"12\",\"2\":\"Calibri\"},{\"1\":\"13\",\"2\":\"Calibri\"},{\"1\":\"14\",\"2\":\"Calibri\"},{\"1\":\"15\",\"2\":\"Calibri\"},{\"1\":\"16\",\"2\":\"Calibri\"},{\"1\":\"17\",\"2\":\"Calibri\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nsusp_fmt <- which(drv_fmt2$local$font$name == \"Cambria\")\r\n\r\n\r\n\r\nThen we locate all the rows in the original spreadsheet with Cambria in the first odometer reading column.\r\n\r\n\r\nsusp_rows <- drv_fmt %>% \r\n    filter(local_format_id == susp_fmt, col == 3) %>% \r\n    distinct(row) %>% pull()\r\n\r\n\r\n\r\nAnd then we create an indicator of the font for each policy (row in the original spreadsheet). Notice that because of the headings the first policy appears in row 2. Therefore, pid links to the row - 1.\r\n\r\n\r\ndrv_long_font <- drv_long %>% \r\n    mutate(cambria = pid %in% (susp_rows - 1))\r\n\r\n\r\n\r\nFinally, we can check for the cumulative distribution function of the miles driven between the Cambria and Calibri fonts. For completeness we will visualize all cars.\r\n\r\n\r\ndrv_long_font %>% \r\n    arrange(pid, car_num, period) %>% \r\n    group_by(pid, cambria, car_num) %>% \r\n    summarize(miles_diff = diff(miles), .groups = \"drop\") %>% \r\n    ggplot(aes(miles_diff, col = cambria)) +\r\n    stat_ecdf(geom = \"step\") +\r\n    facet_wrap(~car_num) +\r\n    scale_x_continuous(labels = comma_format()) +\r\n    scale_y_continuous(labels = percent_format(accuracy = 1)) +\r\n    labs(x = \"Miles driven\", y = \"CDF\",\r\n         title = \"Not exactly what we were expecting\",\r\n         subtitle = \"Cumulative distribution function of miles driven\")\r\n\r\n\r\n\r\n\r\nWait! This is strange. It looks very different than the plot we were trying to replicate. Did you spot the mistake? The Datacolada blogpost Figure 5 title states “CDFs of Miles Driven …” but the x-axis and the data is actually the first odometer reading. Even fraud detection posts can have mistakes (and as written in the disclaimers - also this post). However, the analysis is clear enough so we can replicate the original Figure.\r\n\r\n\r\ndrv_long_font %>% \r\n    filter(period == \"Pre\") %>% \r\n    ggplot(aes(miles, col = cambria, lty = cambria)) +\r\n    stat_ecdf(geom = \"step\", size = 1.5) +\r\n    facet_wrap(~car_num) +\r\n    coord_cartesian(xlim = c(0,1.5e5)) +\r\n    scale_x_continuous(labels = comma_format()) +\r\n    scale_y_continuous(labels = percent_format(accuracy = 1)) +\r\n    labs(x = \"First odometer reading\", y = \"CDF\",\r\n         title = \"A suspicous similiarity in distributions across fonts\",\r\n         subtitle = \"Cumulative distribution function of the first odometer readings\")\r\n\r\n\r\n\r\n\r\nAnomaly #4: No Rounding in Cambria Observations\r\nLastly, we can use the graph created earlier to do the same for number rounding with different fonts. Here also, the absence of obvious number rounding and the inconsistency of the number rounding pattern across fonts suggests a concern regarding the data processing.\r\n\r\n\r\ndrv_long_font %>% \r\n    filter(period == \"Pre\") %>% \r\n    mutate(last_digits = miles %% 1e3) %>% \r\n    mutate(font = if_else(cambria, \"Cambria\", \"Calibri\")) %>% \r\n    group_by(font, last_digits) %>% \r\n    summarize(n = n()) %>% \r\n    mutate(pct = n / sum(n),\r\n           mult_100 = last_digits %% 100 == 0) %>% \r\n    ungroup() %>% \r\n    arrange(mult_100) %>% \r\n    ggplot(aes(last_digits, pct, col = mult_100)) +\r\n    geom_point(show.legend = F) +\r\n    scale_color_manual(values = c(\"black\", \"red\")) +\r\n    scale_y_continuous(labels = percent_format(accuracy = 1)) +\r\n    scale_x_continuous(breaks = c(0,200,400,600,800,999), \r\n                       labels = c(\"000\", 200, 400, 600, 800, 999)) +\r\n    facet_wrap(~font) +\r\n    labs(x = \"Last 3 digits\", y = \"Percent of readings\",\r\n         title = \"Another inconsistency in number rounding\",\r\n         subtitle = \"Percent of reading with specific 3 last digits across fonts\"\r\n         )\r\n\r\n\r\n\r\n\r\nSome lessons for me and maybe for you too.\r\nGeneral lessons\r\nAs written earlier, the aim of this post is not to bash, blame, humiliate, or ridicule anyone. Much appreciation for the authors of the Datacolada blog post for having the courage to conduct the analysis, and for the authors of the papers to share the data.\r\nWhether data was deliberately manipulated is outside the scope of this post and outside of the expertise of the post’s author.\r\nTools from the world of fraud detection can be used to raise suspicion regarding scientific literature. This is extremely important as many studies, on which we base policy worldwide, fail to replicate.\r\nThese types of analyses should be encouraged by all who care for the truth and hope to extract value from scientific literature.\r\nData analysis lessons\r\nData analysis should be carefully conducted. To the least, data should be kept, together with a log of all the processing (optimally with the original script, software and package versions). Ideally (although obviously not always possible), data should be shared together with code.\r\nPrior to, during, and after analysis, assumptions about data types, distributions, means, shapes should be assessed. If it is possible, independently by more than one person.\r\nEven analysis conducted to detect mistakes can have mistakes of its own.\r\nR related lessons\r\ngtsummary is great for fast, easy, pretty statistical tables.\r\ntidying a data frame really makes life much easier. Insist on it.\r\nWith some basic regex knowledge you can save precious time.\r\ntidyxl is a tool for the really hard yet not uncommon nuts.\r\nrmarkdown is fantastic.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-08-21-replicating-a-fraud-detection/replicating-a-fraud-detection_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-08-21T00:44:13+03:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-15-recreating-the-new-york-times-wall-of-grief/",
    "title": "Recreating The New York Times Wall of Grief",
    "description": "A graphic on Sunday’s front page depicted the totality of Covid’s devastation in the USA. I recreate it using R.",
    "author": [
      {
        "name": "Avishai M Tsur",
        "url": {}
      }
    ],
    "date": "2021-04-15",
    "categories": [],
    "contents": "\r\nIntroduction\r\nIn this post, we will recreate a graphic from The New York Times published on Feb 21, 2021. In it, there is a depiction of almost 500K COVID-19 deaths in the USA alone. To be honest, when presenting a time-series it is not often a good idea to show it using this kind of visualization where dot density is conveying the information. But, in this case and context, it is part of what makes this visualization so appealing (and appalling).\r\nDuring the last year, I worked for six months in a COVID-19 ward in my hospital. We have witnessed deaths day in and day out. I also read the mortality statistics obsessively throughout the year. However, this visualization mixed the pain for each individual together with the volume of total deaths in a way that moved me and made me re-think and re-feel. Kudos to the originators.\r\nThroughout this example, we will discuss the principles and try to recreate it with some minor differences. More customization, highlighting, and fine-tuning is possible. However, we’ll focus on just some of the main properties of this visualization to keep it simple.\r\nLoad packages\r\nFirst, let’s load all relevant packages.tidyverse loads many packages used for the bread and butter of data science. For data wrangling, there are other frameworks such as data.table that can also be used.glue is a great package for concatenating (or pasting) strings together.vroom is a package that speeds-up reading CSV files.scales is used to change numbers to a comma format.knitr::kable is used to print the data frame.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(glue)\r\nlibrary(vroom)\r\nlibrary(scales)\r\nlibrary(knitr)\r\n\r\n\r\n\r\nLoad data\r\nCOVID-19 data can be found in many places. Here, we load the data from Our World in Data mostly because it is likely to stay online also years from now.\r\n\r\n\r\ncovid <- vroom(\"https://covid.ourworldindata.org/data/owid-covid-data.csv\")\r\n\r\n\r\n\r\nQuick look\r\n\r\n\r\ncovid %>% \r\n  head() %>% \r\n  select(iso_code, location, date, new_deaths, total_deaths) %>% \r\n  kable()\r\n\r\n\r\niso_code\r\nlocation\r\ndate\r\nnew_deaths\r\ntotal_deaths\r\nAFG\r\nAfghanistan\r\n2020-02-24\r\nNA\r\nNA\r\nAFG\r\nAfghanistan\r\n2020-02-25\r\nNA\r\nNA\r\nAFG\r\nAfghanistan\r\n2020-02-26\r\nNA\r\nNA\r\nAFG\r\nAfghanistan\r\n2020-02-27\r\nNA\r\nNA\r\nAFG\r\nAfghanistan\r\n2020-02-28\r\nNA\r\nNA\r\nAFG\r\nAfghanistan\r\n2020-02-29\r\nNA\r\nNA\r\n\r\nCreating our main dataset\r\nWe can see that in this dataset each row represents a day in a country. For now, we only need data for USA, and only need the date, number of new deaths, and cumulative number of deaths. The plot starts at the date of the first death. Our Y-axis is going to be days till the last date we choose. For the plot, we need to uncount the deaths so each row is an individual who died.\r\n\r\n\r\ncovid_us <- covid %>% \r\n    filter(iso_code == \"USA\") %>% \r\n    select(date, new_deaths, total_deaths) %>% \r\n    filter(total_deaths > 0) %>% \r\n    mutate(days_to_last = n() - row_number()) %>% \r\n    uncount(new_deaths)\r\n\r\n\r\n\r\nDownsampling to accelerate drafts\r\nIn the final plot, we will use all data points. But, as there are over 500K points, and plotting is slow, it quickens the process if we downsample through drafts until we our satisfied with the result. We use identity() here so we could later comment out the downsampling without errors.\r\n\r\n\r\ncovid_us_downsampled <- covid_us %>% \r\n    sample_n(1e4) %>%\r\n    arrange(date) %>%\r\n    identity()\r\n\r\n\r\n\r\nCreating our annotation dataset\r\nIn the original graphic, every milestone of 50K deaths is annotated. To replicate it here we signal every crossing of 50K deaths, and then create the text for annotation–the date and total number of deaths.\r\n\r\n\r\nbreak_points <- covid_us_downsampled %>% \r\n    mutate(deaths_50k = total_deaths %/% 5e4) %>% \r\n    group_by(deaths_50k) %>% \r\n    slice(1) %>% \r\n    ungroup() %>% \r\n    mutate(pos = 0, xmin = 0, xmax = 1,\r\n           date_text = format.Date(date, \"%b %d\"),\r\n           date_text2 = glue(\"{date_text}\\n{comma(total_deaths)}\")\r\n           )\r\n\r\n\r\n\r\nRe-creating the graphic\r\nEvery death is randomly spread across the X-axis. The value on the Y-axis relates to the date. The slight jitter on the Y-axis is to prevent stripes. In this case, annotations are just geoms created with a different dataset. Using some transperancy allows to visualize overlapping dots.\r\n\r\n\r\nplot_draft <- covid_us_downsampled %>% \r\n    mutate(pos = runif(n())) %>% \r\n    ggplot(aes(pos, days_to_last)) +\r\n    geom_jitter(size = 0.2, alpha = 0.2, width = 0, height = 0.5) +\r\n    geom_linerange(data = break_points, aes(xmin = xmin, xmax = xmax, y = days_to_last), col = \"red\") +\r\n    scale_y_continuous(breaks = break_points$days_to_last, labels = break_points$date_text2)\r\n\r\nplot_draft\r\n\r\n\r\n\r\n\r\nThe plot looks fine to begin with, but the axis titles are unneeded, the background is low-contrast, the gridlines are distracting, and the annotations are de-emphasized.\r\n\r\n\r\nplot_downsampled <- plot_draft +\r\n  theme_classic() +\r\n    theme(\r\n        axis.line = element_blank(),\r\n        axis.title = element_blank(),\r\n        axis.text.x = element_blank(),\r\n        axis.ticks = element_blank(),\r\n        axis.text.y = element_text(color = \"red\", hjust = 0, face = \"bold\")\r\n    )\r\n\r\nplot_downsampled\r\n\r\n\r\n\r\n\r\nThis one looks better. It may look a bit anemic but we should remember we only plotted 1K dots and not the whole > 500K. Now that we are happy with the plot, let us plot all of them.\r\n\r\n\r\ncovid_us_downsampled <- covid_us %>% \r\n    # sample_n(1e4) %>%\r\n    # arrange(date) %>%\r\n    identity()\r\n\r\n\r\n\r\n\r\n\r\nbreak_points <- covid_us_downsampled %>% \r\n    mutate(deaths_50k = total_deaths %/% 5e4) %>% \r\n    group_by(deaths_50k) %>% \r\n    slice(1) %>% \r\n    ungroup() %>% \r\n    mutate(pos = 0, xmin = 0, xmax = 1,\r\n           date_text = format.Date(date, \"%b %d\"),\r\n           date_text2 = glue(\"{date_text}\\n{comma(total_deaths)}\")\r\n           )\r\n\r\n\r\n\r\n\r\n\r\nplot_final <- covid_us_downsampled %>% \r\n    mutate(pos = runif(n())) %>% \r\n    ggplot(aes(pos, days_to_last)) +\r\n    geom_jitter(size = 0.2, alpha = 0.2, width = 0, height = 0.5) +\r\n    geom_linerange(data = break_points, aes(xmin = xmin, xmax = xmax, y = days_to_last), col = \"red\") +\r\n    scale_y_continuous(breaks = break_points$days_to_last, labels = break_points$date_text2) +\r\n  theme_classic() +\r\n    theme(\r\n        axis.line = element_blank(),\r\n        axis.title = element_blank(),\r\n        axis.text.x = element_blank(),\r\n        axis.ticks = element_blank(),\r\n        axis.text.y = element_text(color = \"red\", hjust = 0, face = \"bold\")\r\n    )\r\n\r\nplot_final\r\n\r\n\r\n\r\n\r\nKey learning points for me\r\nOne strong simple message can be much more moving than many informative-heavy graphs.\r\nuncount() to imitate individual observations from summaries.\r\nDownsampling to accelerate the workflow until the finishing line.\r\nidentity() to allow commenting out in pipes.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-15-recreating-the-new-york-times-wall-of-grief/recreating-the-new-york-times-wall-of-grief_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2021-04-15T21:49:03+03:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to website",
    "description": "Welcome to my new blog, website. I hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Avishai M Tsur",
        "url": {}
      }
    ],
    "date": "2021-04-15",
    "categories": [],
    "contents": "\r\nThis is my personal website. Here, I will share thoughts and code chunks regarding health, science, and data.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-04-15T21:28:09+03:00",
    "input_file": {}
  }
]
